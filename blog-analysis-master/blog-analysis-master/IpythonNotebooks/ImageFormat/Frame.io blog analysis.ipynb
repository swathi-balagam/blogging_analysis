{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame.io Blog analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bonobo\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "    'referrer': 'https://google.com'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame.io categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['post-production', 'color-correction', 'business', 'workflow', 'behind-the-scenes', 'production', 'announcement']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing page navigator Finding next link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_link(soup_item):\n",
    "    bottom_nav = soup_item.find(class_='navigation')\n",
    "    \n",
    "    if bottom_nav == None:\n",
    "        return None\n",
    "    \n",
    "    links = bottom_nav.findAll('a')\n",
    "    next_page = links[-1]\n",
    "\n",
    "    if next_page.contents[0] == 'Next':\n",
    "        next_link = next_page['href']\n",
    "        return next_link\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse URL and get categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_store = []\n",
    "\n",
    "def parse_category(url):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    html = r.text.strip()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    article_cards = soup.findAll(class_='post-content')\n",
    "\n",
    "    for article in article_cards:\n",
    "        title = article.find(class_='post-meta-title')\n",
    "        link = title.contents[0]['href']\n",
    "        print('Parsing URL:', link)\n",
    "        page = parse_page(link)\n",
    "        articles_store.append(page)\n",
    "        \n",
    "    next_link = find_next_link(soup)\n",
    "    \n",
    "    if next_link is not None:\n",
    "        print('Next page:', next_link)\n",
    "        parse_category(next_link)\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting read time of article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_read_time(header):\n",
    "    html_str = header.find(class_='read-time')\n",
    "    time_str = html_str.contents[0].strip().lower().split()[0]\n",
    "    time_int = int(time_str)\n",
    "    return time_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Title of Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(header):\n",
    "    html_str = header.find(class_='post-meta-title')\n",
    "    title_str = html_str.contents[0].strip()\n",
    "    return title_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Date of blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(header):\n",
    "    html_str = header.find(class_='single-post-date')\n",
    "    date_str = html_str.contents[0].strip()\n",
    "    return date_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract author of Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author(header):\n",
    "    html_str = header.find(class_='author-name')\n",
    "    author_str = html_str.find('a').contents[0].strip()\n",
    "    return author_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories(header):\n",
    "    html_str = header.find(class_='single-post-cat')\n",
    "    categories = html_str.findAll('a')\n",
    "    cat_names = []\n",
    "    for cat_link in categories:\n",
    "        cat_name = cat_link.contents[0].strip().lower()\n",
    "        cat_names.append(cat_name)\n",
    "    return cat_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parsing page URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_page(url):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    html = r.text.strip()\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # Header Content\n",
    "    header = soup.find(class_='entry-header')\n",
    "    read_time = extract_read_time(header)\n",
    "    title = extract_title(header)\n",
    "\n",
    "    author = extract_author(header)\n",
    "    categories = extract_categories(header)\n",
    "\n",
    "    date = extract_date(header)\n",
    "    dt = parser.parse(date)\n",
    "    month = dt.strftime(\"%B\")\n",
    "    weekday = dt.strftime(\"%A\")\n",
    "    \n",
    "    # Body Content\n",
    "    content = soup.find(class_='entry-content')\n",
    "    word_count = len(content.text.split())\n",
    "    reading_level = textstat.flesch_kincaid_grade(content.text)\n",
    "\n",
    "    links = content.find_all(\"a\")\n",
    "    link_count = len(links)\n",
    "\n",
    "    images = content.find_all(\"img\")\n",
    "    image_count = len(images)\n",
    "    \n",
    "    page_data = {\n",
    "        'reading_time' : read_time,\n",
    "        'title': title,\n",
    "        'date': date,\n",
    "        'month': month,\n",
    "        'weekday': weekday,\n",
    "        'author': author,\n",
    "        'categories': categories,\n",
    "        'word_count': word_count,\n",
    "        'reading_level': reading_level,\n",
    "        'link_count': link_count,\n",
    "        'image_count': image_count\n",
    "    }\n",
    "    \n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
